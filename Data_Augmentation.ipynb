{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "provenance": [],
   "authorship_tag": "ABX9TyOAGakAVu1Ik6gFW5vg+t5n",
   "include_colab_link": true
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3 (ipykernel)",
   "language": "python"
  },
  "language_info": {
   "name": "python"
  }
 },
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Import Libraries"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:45:07.012893Z",
     "start_time": "2025-04-17T08:44:58.451927Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, datasets, models\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score\n"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load and Split Dataset"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:45:08.557142Z",
     "start_time": "2025-04-17T08:45:08.552690Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Set device\n",
    "device = torch.device(\"cuda\")"
   ],
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:51:08.711251Z",
     "start_time": "2025-04-17T08:51:08.671886Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load the multilabel data\n",
    "data = pd.read_csv('LandUse_Multilabeled.txt', sep='\\t')\n",
    "X = data.iloc[:, 0]  # Image names\n",
    "y = data.iloc[:, 1:]  # Labels\n",
    "\n",
    "# Split into train, validation, and test sets\n",
    "# No stratification is applied because it gave a message that some classes only occur once\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "X_test, X_val, y_test, y_val = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n"
   ],
   "outputs": [],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:51:11.116807Z",
     "start_time": "2025-04-17T08:51:11.101003Z"
    }
   },
   "cell_type": "code",
   "source": "num_labels = len(data.iloc[0, :])-1",
   "outputs": [],
   "execution_count": 5
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:51:13.174780Z",
     "start_time": "2025-04-17T08:51:13.156053Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(\"Train size: \", len(X_train))\n",
    "print(\"Validation size: \", len(X_val))\n",
    "print(\"Test size: \", len(X_test))"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size:  1470\n",
      "Validation size:  315\n",
      "Test size:  315\n"
     ]
    }
   ],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Define Dataset Class"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:51:19.760934Z",
     "start_time": "2025-04-17T08:51:19.754708Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class UCMercedDataset(Dataset):\n",
    "    def __init__(self, image_names, labels, image_dir, transform=None):\n",
    "        self.image_names = image_names\n",
    "        self.labels = labels\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_names)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        base_filename = self.image_names.iloc[idx]\n",
    "        subfolder = base_filename[:-2]     # 'airplane22' -> 'airplane'\n",
    "\n",
    "        img_path = os.path.join(\"Images\", subfolder, f\"{base_filename}.tif\")\n",
    "\n",
    "        if not os.path.exists(img_path):\n",
    "            raise FileNotFoundError(f\"Image not found at: {img_path}\")\n",
    "\n",
    "        image = datasets.folder.default_loader(img_path)\n",
    "        label = torch.tensor(self.labels.iloc[idx].values, dtype=torch.float32)\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image, label\n"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Preprocess Images"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:51:23.537795Z",
     "start_time": "2025-04-17T08:51:23.528255Z"
    }
   },
   "cell_type": "code",
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = UCMercedDataset(X_train, y_train, 'Images', transform)\n",
    "val_dataset = UCMercedDataset(X_val, y_val, 'Images', transform)\n",
    "test_dataset = UCMercedDataset(X_test, y_test, 'Images', transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=32, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)\n"
   ],
   "outputs": [],
   "execution_count": 8
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Load and Adjust Pretrained Models"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:51:29.105267Z",
     "start_time": "2025-04-17T08:51:28.107977Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Load DINO v2 ResNet 50\n",
    "resnet50 = torch.hub.load('facebookresearch/dino:main', 'dino_resnet50')\n",
    "\n",
    "# Load DINO v2 ViT-S/8\n",
    "vits8 = torch.hub.load('facebookresearch/dino:main', 'dino_vits8')"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in C:\\Users\\tiesk/.cache\\torch\\hub\\facebookresearch_dino_main\n",
      "C:\\Users\\tiesk\\PycharmProjects\\DeepLearningMGI12\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "C:\\Users\\tiesk\\PycharmProjects\\DeepLearningMGI12\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
      "  warnings.warn(msg)\n",
      "Using cache found in C:\\Users\\tiesk/.cache\\torch\\hub\\facebookresearch_dino_main\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:51:40.412146Z",
     "start_time": "2025-04-17T08:51:40.395582Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Change the Dino model to multi-label classification\n",
    "class DinoResNetMultiLabel(nn.Module):\n",
    "    def __init__(self, base_model, num_labels):\n",
    "        super().__init__()\n",
    "        self.backbone = nn.Sequential(*list(base_model.children())[:-1])  # remove final classification layer\n",
    "        self.classifier = nn.Linear(2048, num_labels)  # 2048 is the output feature dim of ResNet50\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.backbone(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.classifier(x)\n",
    "        return x"
   ],
   "outputs": [],
   "execution_count": 10
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:51:49.460548Z",
     "start_time": "2025-04-17T08:51:49.452283Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Change the ViT model to multi-label classification\n",
    "class DinoViTMultiLabel(nn.Module):\n",
    "    def __init__(self, base_model, num_labels):\n",
    "        super().__init__()\n",
    "        self.backbone = base_model\n",
    "        self.classifier = nn.Linear(base_model.embed_dim, num_labels)  # embed_dim is the output feature dim of ViT-S/8\n",
    "\n",
    "    def forward(self, x):\n",
    "        features = self.backbone(x)\n",
    "        return self.classifier(features)"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prepare for training"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:52:04.090971Z",
     "start_time": "2025-04-17T08:52:03.913199Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Model setup\n",
    "resnet_model = DinoResNetMultiLabel(resnet50, num_labels).to(device)\n",
    "vits_model = DinoViTMultiLabel(vits8, num_labels).to(device)\n",
    "\n",
    "# Loss and optimizers\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "resnet_optimizer = torch.optim.Adam(resnet_model.parameters(), lr=1e-4)\n",
    "vits_optimizer = torch.optim.Adam(vits_model.parameters(), lr=1e-4)"
   ],
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Define Training Loop"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T08:52:06.720262Z",
     "start_time": "2025-04-17T08:52:06.713617Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Training function\n",
    "def train_model(model, optimizer, name):\n",
    "    for epoch in range(10):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for images, labels in train_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"[{name}] Epoch {epoch+1}: Train Loss = {avg_loss:.4f}\")\n",
    "\n",
    "        validate_model(model, name)\n",
    "\n",
    "# Validation function\n",
    "def validate_model(model, name):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in val_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = torch.sigmoid(model(images))\n",
    "            preds = (outputs > 0.5).int()\n",
    "\n",
    "            all_preds.append(preds.cpu())\n",
    "            all_targets.append(labels.cpu().int())\n",
    "\n",
    "    preds = torch.cat(all_preds)\n",
    "    targets = torch.cat(all_targets)\n",
    "\n",
    "    f1 = f1_score(targets, preds, average=\"macro\", zero_division=0)\n",
    "    print(f\"[{name}] Validation F1 score: {f1:.4f}\\n\")"
   ],
   "outputs": [],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Train and Evaluate Models"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-17T09:00:01.959755Z",
     "start_time": "2025-04-17T08:52:10.925304Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Train ResNet 50\n",
    "print(\"Training ResNet 50...\")\n",
    "train_model(resnet_model, resnet_optimizer, name=\"ResNet50\")"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training ResNet 50...\n",
      "[ResNet50] Epoch 1: Train Loss = 0.5360\n",
      "[ResNet50] Validation F1 score: 0.2895\n",
      "\n",
      "[ResNet50] Epoch 2: Train Loss = 0.3222\n",
      "[ResNet50] Validation F1 score: 0.4852\n",
      "\n",
      "[ResNet50] Epoch 3: Train Loss = 0.2248\n",
      "[ResNet50] Validation F1 score: 0.5975\n",
      "\n",
      "[ResNet50] Epoch 4: Train Loss = 0.1740\n",
      "[ResNet50] Validation F1 score: 0.7227\n",
      "\n",
      "[ResNet50] Epoch 5: Train Loss = 0.1462\n",
      "[ResNet50] Validation F1 score: 0.8683\n",
      "\n",
      "[ResNet50] Epoch 6: Train Loss = 0.1232\n",
      "[ResNet50] Validation F1 score: 0.9016\n",
      "\n",
      "[ResNet50] Epoch 7: Train Loss = 0.1044\n",
      "[ResNet50] Validation F1 score: 0.9151\n",
      "\n",
      "[ResNet50] Epoch 8: Train Loss = 0.0921\n",
      "[ResNet50] Validation F1 score: 0.9237\n",
      "\n",
      "[ResNet50] Epoch 9: Train Loss = 0.0817\n",
      "[ResNet50] Validation F1 score: 0.9303\n",
      "\n",
      "[ResNet50] Epoch 10: Train Loss = 0.0678\n",
      "[ResNet50] Validation F1 score: 0.9289\n",
      "\n",
      "Training ViT-S/8...\n"
     ]
    },
    {
     "ename": "OutOfMemoryError",
     "evalue": "CUDA out of memory. Tried to allocate 452.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 9.88 GiB is allocated by PyTorch, and 752.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)",
     "output_type": "error",
     "traceback": [
      "\u001B[31m---------------------------------------------------------------------------\u001B[39m",
      "\u001B[31mOutOfMemoryError\u001B[39m                          Traceback (most recent call last)",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[14]\u001B[39m\u001B[32m, line 7\u001B[39m\n\u001B[32m      5\u001B[39m \u001B[38;5;66;03m# Train ViT-S/8\u001B[39;00m\n\u001B[32m      6\u001B[39m \u001B[38;5;28mprint\u001B[39m(\u001B[33m\"\u001B[39m\u001B[33mTraining ViT-S/8...\u001B[39m\u001B[33m\"\u001B[39m)\n\u001B[32m----> \u001B[39m\u001B[32m7\u001B[39m \u001B[43mtrain_model\u001B[49m\u001B[43m(\u001B[49m\u001B[43mvits_model\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mvits_optimizer\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43mname\u001B[49m\u001B[43m=\u001B[49m\u001B[33;43m\"\u001B[39;49m\u001B[33;43mViT-S/8\u001B[39;49m\u001B[33;43m\"\u001B[39;49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[13]\u001B[39m\u001B[32m, line 11\u001B[39m, in \u001B[36mtrain_model\u001B[39m\u001B[34m(model, optimizer, name)\u001B[39m\n\u001B[32m      8\u001B[39m images, labels = images.to(device), labels.to(device)\n\u001B[32m     10\u001B[39m optimizer.zero_grad()\n\u001B[32m---> \u001B[39m\u001B[32m11\u001B[39m outputs = \u001B[43mmodel\u001B[49m\u001B[43m(\u001B[49m\u001B[43mimages\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     12\u001B[39m loss = criterion(outputs, labels)\n\u001B[32m     13\u001B[39m loss.backward()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DeepLearningMGI12\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DeepLearningMGI12\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mCell\u001B[39m\u001B[36m \u001B[39m\u001B[32mIn[11]\u001B[39m\u001B[32m, line 9\u001B[39m, in \u001B[36mDinoViTMultiLabel.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m      8\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x):\n\u001B[32m----> \u001B[39m\u001B[32m9\u001B[39m     features = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mbackbone\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m     10\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m.classifier(features)\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DeepLearningMGI12\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DeepLearningMGI12\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.cache\\torch\\hub\\facebookresearch_dino_main\\vision_transformer.py:212\u001B[39m, in \u001B[36mVisionTransformer.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m    210\u001B[39m x = \u001B[38;5;28mself\u001B[39m.prepare_tokens(x)\n\u001B[32m    211\u001B[39m \u001B[38;5;28;01mfor\u001B[39;00m blk \u001B[38;5;129;01min\u001B[39;00m \u001B[38;5;28mself\u001B[39m.blocks:\n\u001B[32m--> \u001B[39m\u001B[32m212\u001B[39m     x = \u001B[43mblk\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    213\u001B[39m x = \u001B[38;5;28mself\u001B[39m.norm(x)\n\u001B[32m    214\u001B[39m \u001B[38;5;28;01mreturn\u001B[39;00m x[:, \u001B[32m0\u001B[39m]\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DeepLearningMGI12\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DeepLearningMGI12\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.cache\\torch\\hub\\facebookresearch_dino_main\\vision_transformer.py:108\u001B[39m, in \u001B[36mBlock.forward\u001B[39m\u001B[34m(self, x, return_attention)\u001B[39m\n\u001B[32m    107\u001B[39m \u001B[38;5;28;01mdef\u001B[39;00m\u001B[38;5;250m \u001B[39m\u001B[34mforward\u001B[39m(\u001B[38;5;28mself\u001B[39m, x, return_attention=\u001B[38;5;28;01mFalse\u001B[39;00m):\n\u001B[32m--> \u001B[39m\u001B[32m108\u001B[39m     y, attn = \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mattn\u001B[49m\u001B[43m(\u001B[49m\u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43mnorm1\u001B[49m\u001B[43m(\u001B[49m\u001B[43mx\u001B[49m\u001B[43m)\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m    109\u001B[39m     \u001B[38;5;28;01mif\u001B[39;00m return_attention:\n\u001B[32m    110\u001B[39m         \u001B[38;5;28;01mreturn\u001B[39;00m attn\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DeepLearningMGI12\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1739\u001B[39m, in \u001B[36mModule._wrapped_call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1737\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28mself\u001B[39m._compiled_call_impl(*args, **kwargs)  \u001B[38;5;66;03m# type: ignore[misc]\u001B[39;00m\n\u001B[32m   1738\u001B[39m \u001B[38;5;28;01melse\u001B[39;00m:\n\u001B[32m-> \u001B[39m\u001B[32m1739\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[38;5;28;43mself\u001B[39;49m\u001B[43m.\u001B[49m\u001B[43m_call_impl\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~\\PycharmProjects\\DeepLearningMGI12\\.venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1750\u001B[39m, in \u001B[36mModule._call_impl\u001B[39m\u001B[34m(self, *args, **kwargs)\u001B[39m\n\u001B[32m   1745\u001B[39m \u001B[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001B[39;00m\n\u001B[32m   1746\u001B[39m \u001B[38;5;66;03m# this function, and just call forward.\u001B[39;00m\n\u001B[32m   1747\u001B[39m \u001B[38;5;28;01mif\u001B[39;00m \u001B[38;5;129;01mnot\u001B[39;00m (\u001B[38;5;28mself\u001B[39m._backward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_hooks \u001B[38;5;129;01mor\u001B[39;00m \u001B[38;5;28mself\u001B[39m._forward_pre_hooks\n\u001B[32m   1748\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_backward_pre_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_backward_hooks\n\u001B[32m   1749\u001B[39m         \u001B[38;5;129;01mor\u001B[39;00m _global_forward_hooks \u001B[38;5;129;01mor\u001B[39;00m _global_forward_pre_hooks):\n\u001B[32m-> \u001B[39m\u001B[32m1750\u001B[39m     \u001B[38;5;28;01mreturn\u001B[39;00m \u001B[43mforward_call\u001B[49m\u001B[43m(\u001B[49m\u001B[43m*\u001B[49m\u001B[43margs\u001B[49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m*\u001B[49m\u001B[43m*\u001B[49m\u001B[43mkwargs\u001B[49m\u001B[43m)\u001B[49m\n\u001B[32m   1752\u001B[39m result = \u001B[38;5;28;01mNone\u001B[39;00m\n\u001B[32m   1753\u001B[39m called_always_called_hooks = \u001B[38;5;28mset\u001B[39m()\n",
      "\u001B[36mFile \u001B[39m\u001B[32m~/.cache\\torch\\hub\\facebookresearch_dino_main\\vision_transformer.py:85\u001B[39m, in \u001B[36mAttention.forward\u001B[39m\u001B[34m(self, x)\u001B[39m\n\u001B[32m     82\u001B[39m qkv = \u001B[38;5;28mself\u001B[39m.qkv(x).reshape(B, N, \u001B[32m3\u001B[39m, \u001B[38;5;28mself\u001B[39m.num_heads, C // \u001B[38;5;28mself\u001B[39m.num_heads).permute(\u001B[32m2\u001B[39m, \u001B[32m0\u001B[39m, \u001B[32m3\u001B[39m, \u001B[32m1\u001B[39m, \u001B[32m4\u001B[39m)\n\u001B[32m     83\u001B[39m q, k, v = qkv[\u001B[32m0\u001B[39m], qkv[\u001B[32m1\u001B[39m], qkv[\u001B[32m2\u001B[39m]\n\u001B[32m---> \u001B[39m\u001B[32m85\u001B[39m attn = (\u001B[43mq\u001B[49m\u001B[43m \u001B[49m\u001B[43m@\u001B[49m\u001B[43m \u001B[49m\u001B[43mk\u001B[49m\u001B[43m.\u001B[49m\u001B[43mtranspose\u001B[49m\u001B[43m(\u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m2\u001B[39;49m\u001B[43m,\u001B[49m\u001B[43m \u001B[49m\u001B[43m-\u001B[49m\u001B[32;43m1\u001B[39;49m\u001B[43m)\u001B[49m) * \u001B[38;5;28mself\u001B[39m.scale\n\u001B[32m     86\u001B[39m attn = attn.softmax(dim=-\u001B[32m1\u001B[39m)\n\u001B[32m     87\u001B[39m attn = \u001B[38;5;28mself\u001B[39m.attn_drop(attn)\n",
      "\u001B[31mOutOfMemoryError\u001B[39m: CUDA out of memory. Tried to allocate 452.00 MiB. GPU 0 has a total capacity of 4.00 GiB of which 0 bytes is free. Of the allocated memory 9.88 GiB is allocated by PyTorch, and 752.73 MiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Train ViT-S/8\n",
    "print(\"Training ViT-S/8...\")\n",
    "train_model(vits_model, vits_optimizer, name=\"ViT-S/8\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Evaluate Models"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "'''\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = torch.sigmoid(model(images))\n",
    "            preds = (outputs > 0.5).float()\n",
    "            all_labels.append(labels.cpu())\n",
    "            all_preds.append(preds.cpu())\n",
    "\n",
    "    all_labels = torch.cat(all_labels)\n",
    "    all_preds = torch.cat(all_preds)\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    precision = precision_score(all_labels, all_preds, average='macro')\n",
    "    recall = recall_score(all_labels, all_preds, average='macro')\n",
    "    print(f\"F1 Score: {f1:.4f}, Precision: {precision:.4f}, Recall: {recall:.4f}\")\n",
    "\n",
    "# Evaluate ResNet 50\n",
    "print(\"Evaluating ResNet 50...\")\n",
    "evaluate_model(resnet50, test_loader)\n",
    "\n",
    "# Evaluate ViT-S/8\n",
    "print(\"Evaluating ViT-S/8...\")\n",
    "evaluate_model(vit_s8, test_loader)\n",
    "'''"
   ]
  }
 ]
}
